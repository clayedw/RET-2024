{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clayedw/RET-2024/blob/main/BNN_ideas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "Bayesian neural networks (BNNs) are a subset of probabilistic neural networks that incorporate Bayesian inference principles to model uncertainty in predictions. BNNs estimate distributions over weights, which allows them to capture the uncertainty in predictions more effectively compared to traditional neural networks. This probabilistic approach is particularly useful in fields where understanding the confidence of predictions is crucial.\n",
        "\n",
        "The primary challenge historically has been the computational complexity and resource demands of BNNs, which limited their application to large-scale problems like photo-z estimation in cosmological surveys. However, recent advancements in both theoretical understanding and computational power have made it feasible to apply BNNs to such large-scale data.\n",
        "\n",
        "### Key Advantages of Bayesian Neural Networks\n",
        "\n",
        "1. **Better Uncertainty Representation**:\n",
        "    - Traditional neural networks provide point estimates without quantifying the uncertainty of predictions. BNNs, on the other hand, can provide a distribution over possible outcomes, offering a richer and more informative prediction.\n",
        "    - This capability is crucial in cosmology, where precise estimates of uncertainties can significantly impact the interpretation of results and subsequent theoretical models.\n",
        "\n",
        "2. **Improved Point Predictions**:\n",
        "    - By modeling uncertainties, BNNs can potentially produce more accurate predictions. The inclusion of prior information and the Bayesian framework allows the network to generalize better, especially in cases where data might be sparse or noisy.\n",
        "    - This improvement in prediction accuracy can enhance the reliability of photometric redshift (photo-z) estimates, which are essential for cosmological analyses.\n",
        "\n",
        "3. **Enhanced Interpretability**:\n",
        "    - The probabilistic nature of BNNs aligns with Bayesian inference, which has a long history of development and application in statistical analysis. This alignment provides a framework for understanding and interpreting the workings and outputs of neural networks through established probabilistic methods.\n",
        "    - This interpretability is vital in cosmology, where understanding the model's behavior and the reasons behind predictions can lead to more informed scientific conclusions.\n",
        "\n",
        "### Potential Applications\n",
        "\n",
        "1. **Photometric Redshift Estimation**:\n",
        "    - One of the direct applications of BNNs in cosmology is the estimation of photometric redshifts. Accurate photo-z estimation is critical for large-scale surveys like LSST (Large Synoptic Survey Telescope) and Euclid, which aim to map the distribution of galaxies and study dark energy.\n",
        "    - BNNs can improve the accuracy and reliability of photo-z estimates, leading to better constraints on cosmological parameters.\n",
        "\n",
        "2. **Cosmological Parameter Inference**:\n",
        "    - BNNs can be employed in the analysis of large-scale structure and cosmic microwave background data to infer cosmological parameters. Their ability to quantify uncertainty can provide more robust parameter estimates and help in understanding the underlying physics of the universe.\n",
        "    - For instance, parameters such as the matter density (Ω_m), dark energy equation of state (w), and the amplitude of density fluctuations (σ8) can be better constrained using BNNs.\n",
        "\n",
        "3. **Galaxy Classification and Morphology Studies**:\n",
        "    - In addition to redshift estimation, BNNs can be used for classifying galaxies and studying their morphologies. The probabilistic outputs of BNNs can help in distinguishing between different galaxy types with varying degrees of confidence, leading to more nuanced and accurate classifications.\n",
        "\n",
        "4. **Weak Lensing and Large-Scale Structure Analysis**:\n",
        "    - Weak gravitational lensing is another area where BNNs can be beneficial. Accurate modeling of the lensing signal and its uncertainties is crucial for studying the distribution of dark matter and understanding the growth of cosmic structures.\n",
        "    - BNNs can improve the analysis of weak lensing data by providing more precise and uncertainty-aware models of the lensing effects.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The integration of Bayesian neural networks in cosmology represents a significant advancement in data analysis methodologies. By leveraging the probabilistic nature of BNNs, cosmologists can achieve better uncertainty representation, improved point predictions, and enhanced interpretability of models. These advantages can lead to more accurate and reliable results in various cosmological applications, from photometric redshift estimation to galaxy classification and weak lensing studies. The ongoing developments in computational capabilities and probabilistic deep learning promise a bright future for the application of BNNs in cosmology."
      ],
      "metadata": {
        "id": "bqagyfBXSNEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up Bayesian Neural Networks (BNNs) involves a series of steps that integrate Bayesian inference principles into traditional neural network architectures. Here's a general outline of how BNNs are constructed and implemented:\n",
        "\n",
        "### Steps to Set Up Bayesian Neural Networks\n",
        "\n",
        "1. **Define the Neural Network Architecture**:\n",
        "    - Similar to traditional neural networks, start by defining the architecture of your network. This includes specifying the number of layers, types of layers (e.g., dense, convolutional), activation functions, etc.\n",
        "    - For example, a simple feedforward neural network might be constructed with an input layer, a few hidden layers, and an output layer.\n",
        "\n",
        "2. **Introduce Probabilistic Weights**:\n",
        "    - Unlike traditional neural networks that use deterministic weights, BNNs model the weights as random variables with probability distributions.\n",
        "    - Typically, weights are initialized with prior distributions such as Gaussian distributions. This means that each weight \\( w \\) in the network has a prior probability distribution \\( P(w) \\).\n",
        "\n",
        "3. **Bayesian Inference for Weight Updates**:\n",
        "    - During training, BNNs update the distributions over the weights instead of single weight values. This involves calculating the posterior distribution of the weights given the data.\n",
        "    - The posterior distribution \\( P(w | D) \\) is updated using Bayes' theorem: \\( P(w | D) \\propto P(D | w) P(w) \\), where \\( P(D | w) \\) is the likelihood of the data given the weights and \\( P(w) \\) is the prior distribution of the weights.\n",
        "\n",
        "4. **Approximate Inference Techniques**:\n",
        "    - Exact Bayesian inference is often computationally infeasible for neural networks, so approximation techniques are used. Common methods include:\n",
        "        - **Variational Inference (VI)**: Approximate the true posterior with a simpler distribution and optimize the parameters of this distribution to be close to the true posterior.\n",
        "        - **Monte Carlo Dropout**: Use dropout during training and inference as an approximation to Bayesian inference.\n",
        "        - **Hamiltonian Monte Carlo (HMC)**: A Markov Chain Monte Carlo method that uses Hamiltonian dynamics to sample from the posterior distribution.\n",
        "\n",
        "5. **Implementing the BNN**:\n",
        "    - Popular deep learning frameworks like TensorFlow Probability, PyTorch, and Pyro provide tools and libraries to implement BNNs.\n",
        "    - For example, in TensorFlow Probability, you can use the `tfp.layers.DenseFlipout` layer to create a Bayesian dense layer with variational inference.\n",
        "\n",
        "6. **Training the BNN**:\n",
        "    - Train the BNN using a suitable optimization algorithm. The objective is to minimize the negative log-likelihood (or equivalently, maximize the likelihood) and the KL-divergence between the approximate posterior and the prior.\n",
        "    - Loss functions in BNNs often combine data likelihood and a regularization term derived from the KL-divergence.\n",
        "\n",
        "7. **Inference and Prediction**:\n",
        "    - During inference, BNNs provide a distribution over the predictions, not just point estimates. This allows for uncertainty quantification in the predictions.\n",
        "    - You can obtain predictive distributions by sampling from the posterior distributions of the weights and averaging the predictions over these samples.\n",
        "\n",
        "### Example Code Snippet Using TensorFlow Probability\n",
        "\n",
        "Here's an example of how to set up a simple Bayesian neural network using TensorFlow Probability:\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "    tfp.layers.DenseFlipout(128, activation='relu'),\n",
        "    tfp.layers.DenseFlipout(128, activation='relu'),\n",
        "    tfp.layers.DenseFlipout(output_dim)\n",
        "])\n",
        "\n",
        "# Define the loss function (negative log-likelihood)\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=negloglik)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model(x_test)\n",
        "```\n",
        "\n",
        "### Potential Applications\n",
        "\n",
        "BNNs can be used in various applications where understanding prediction uncertainty is critical. Some potential applications in cosmology and astrophysics include:\n",
        "\n",
        "1. **Photometric Redshift Estimation**:\n",
        "    - Predict the redshifts of galaxies from photometric data with uncertainty estimates, improving the accuracy of large-scale structure surveys.\n",
        "\n",
        "2. **Galaxy Classification**:\n",
        "    - Classify galaxies into different types while providing uncertainty estimates, helping to understand the reliability of classifications.\n",
        "\n",
        "3. **Supernova Classification**:\n",
        "    - Classify supernovae events with confidence levels, aiding in the study of dark energy and the expansion rate of the universe.\n",
        "\n",
        "4. **Weak Lensing Analysis**:\n",
        "    - Analyze weak gravitational lensing signals with uncertainties, improving the measurement of dark matter distribution.\n",
        "\n",
        "By incorporating Bayesian principles, BNNs enhance the interpretability and reliability of neural network predictions, making them particularly valuable for scientific applications where uncertainty plays a crucial role."
      ],
      "metadata": {
        "id": "UYtQRhGZR0VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Model Architecture Definition and Setup\n",
        "To align with the goals of the project, we need a Bayesian Neural Network (BNN) that accurately predicts photometric redshifts (photo-z) while providing robust uncertainty estimates. Here’s a detailed breakdown of how the model architecture is defined and how it can be set up to meet the specified science goals.\n",
        "\n",
        "###Model Architecture\n",
        "####Input Layer:\n",
        "\n",
        "The input layer is defined by specifying the shape of the input data. For example, if your input data has 10 features, the input shape will be (10,).\n",
        "\n",
        "This layer handles the features extracted from galaxy images, such as magnitudes in different bands."
      ],
      "metadata": {
        "id": "XUukDyWKz7x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 6  # Example number of features\n",
        "input_layer = tf.keras.layers.InputLayer(input_shape=(input_dim,))"
      ],
      "metadata": {
        "id": "VcsNWGYoK5Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dense Layers with Flipout:\n",
        "\n",
        "**tfp.layers.DenseFlipout** is a layer provided by TensorFlow Probability that implements Bayesian inference using Flipout, a method for variational inference. Each layer's number of units is based on the complexity of the data.\n",
        "\n",
        "The 128 in tfp.layers.DenseFlipout(128, activation='relu') specifies the number of units (neurons) in the dense layer.\n",
        "\n",
        "**Activation Functions:** The activation function introduces non-linearity to the model. Different activation functions can be used depending on the nature of the problem. For instance, tanh, sigmoid, or elu can be used instead of relu.\n",
        "\n",
        "**Flipout layers** maintain a distribution over the weights, allowing for uncertainty estimation.\n"
      ],
      "metadata": {
        "id": "ZBtKClKCK7lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense_layer1 = tfp.layers.DenseFlipout(128, activation='relu')\n",
        "dense_layer2 = tfp.layers.DenseFlipout(128, activation='relu')"
      ],
      "metadata": {
        "id": "aI7xzliZLC-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output Layer:\n",
        "The output layer's dimensions depend on the problem. For a regression task, the output dimension is typically 1. For a classification task with N classes, the output dimension is N.\n",
        "\n",
        "The output layer has a single unit for the predicted redshift."
      ],
      "metadata": {
        "id": "yaYCY4fXLGJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tfp.layers.DenseFlipout(1)"
      ],
      "metadata": {
        "id": "Rp8Xci7gLFoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Compilation:\n",
        "\n",
        "The model is compiled with an appropriate loss function for Bayesian neural networks."
      ],
      "metadata": {
        "id": "U3YuhRumLOQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "model.compile(optimizer='adam', loss=negloglik)"
      ],
      "metadata": {
        "id": "MK3CcchWLQxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Model Example"
      ],
      "metadata": {
        "id": "lMUHR_X2MS8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "input_dim = 10  # Example feature size\n",
        "output_dim = 1  # Single output for redshift\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "    tfp.layers.DenseFlipout(128, activation='relu'),\n",
        "    tfp.layers.DenseFlipout(128, activation='relu'),\n",
        "    tfp.layers.DenseFlipout(output_dim)\n",
        "])\n",
        "\n",
        "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
        "model.compile(optimizer='adam', loss=negloglik)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1z9hxDeMRzm",
        "outputId": "276aa408-5c42-4114-937b-c5111aac8ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setting Up for Science Goals\n",
        "The primary science goals involve accurate and precise photometric redshift estimation, requiring the model to meet specific metrics such as RMS error, bias, and outlier rates. Here’s how the model can be tailored:\n",
        "\n",
        "###Photometric Redshift Estimation:\n",
        "\n",
        "Input Features: Magnitudes in different bands.\n",
        "\n",
        "Output: Predicted redshift with uncertainty.\n",
        "Layers: Adjust the number of layers and units to improve model capacity.\n",
        "Activation Functions: Use ReLU for non-linearity.\n",
        "Model Evaluation:\n",
        "\n",
        "Evaluate the model using LSST science requirements (RMS error, bias, 3σ outliers).\n",
        "Use additional metrics like outliers, catastrophic outliers, scatter, and loss for comprehensive evaluation.\n",
        "Metrics are calculated both across the entire redshift range and within specific tomographic bins.\n",
        "###Uncertainty Quantification:\n",
        "\n",
        "Bayesian neural networks naturally provide uncertainty estimates, crucial for understanding model confidence in redshift predictions.\n",
        "###Tomographic Redshift Bins:\n",
        "\n",
        "Divide the redshift range into bins (e.g., 0.2 < z < 1.2 divided into five bins).\n",
        "Ensure the model meets performance metrics within each bin, not just on average across the entire dataset.\n",
        "##Training and Evaluation\n",
        "###Training:\n",
        "\n",
        "Use a sufficiently large training set to capture the complexity of the data.\n",
        "Implement early stopping and regularization to prevent overfitting.\n",
        "Evaluation:\n",
        "\n",
        "Use a validation set to tune hyperparameters and evaluate model performance.\n",
        "Calculate metrics for each tomographic bin to ensure the model meets the requirements across different redshift ranges.\n",
        "###Probabilistic Metrics:\n",
        "\n",
        "Evaluate probabilistic metrics to assess the quality of the uncertainty estimates provided by the BNN.\n",
        "These metrics include calibration curves, sharpness, and coverage probabilities.\n",
        "##Example Training and Evaluation Code"
      ],
      "metadata": {
        "id": "r_oHOBJQLTmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume x_train, y_train are the training data and labels\n",
        "\n",
        "# Training the model\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred = model.predict(x_test)\n",
        "rms_error = np.sqrt(np.mean((y_test - y_pred)**2))\n",
        "bias = np.mean(y_test - y_pred)\n",
        "outliers = np.sum(np.abs(y_test - y_pred) > 3 * rms_error) / len(y_test)\n",
        "\n",
        "print(f'RMS Error: {rms_error}')\n",
        "print(f'Bias: {bias}')\n",
        "print(f'3σ Outliers: {outliers}')\n"
      ],
      "metadata": {
        "id": "_tT3Ip8ELypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Application and Impact\n",
        "Accurately predicting photometric redshifts with robust uncertainty estimates is crucial for:\n",
        "\n",
        "Constraining Dark Matter and Dark Energy: High-precision photo-z estimates allow better measurement of cosmological parameters.\n",
        "\n",
        "\n",
        "Large-Scale Surveys: Ensuring accurate redshift predictions across large datasets like LSST is essential for deriving reliable cosmological insights.\n",
        "\n",
        "\n",
        "Weak Lensing Analyses: Precise redshift estimates improve the accuracy of weak lensing measurements, critical for understanding the distribution of dark matter.\n",
        "\n",
        "\n",
        "Scientific Robustness: By providing uncertainty estimates, BNNs enhance the interpretability and reliability of photometric redshift predictions, crucial for large-scale cosmological surveys."
      ],
      "metadata": {
        "id": "C9WRyZzgL7Ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incorporating a Bayesian Neural Network (BNN) for photometric redshift estimation can significantly enhance the precision and reliability of the redshift measurements. BNNs provide a principled way to quantify uncertainty, which is crucial in cosmological analyses.\n",
        "\n",
        "Here’s a step-by-step approach to incorporating a BNN into your project:\n",
        "\n",
        "### 1. Define the Problem and Prepare the Data\n",
        "Firstly, we need to prepare the training data for the BNN. This typically involves a dataset of galaxies with known spectroscopic redshifts and their corresponding photometric measurements.\n",
        "\n",
        "### 2. Build the Bayesian Neural Network\n",
        "We will use a library like `TensorFlow Probability` to build and train the BNN.\n",
        "\n",
        "### 3. Train the BNN\n",
        "Train the BNN on the prepared dataset. The BNN will learn to predict redshifts from photometric data and estimate the uncertainties.\n",
        "\n",
        "### 4. Predict Redshifts and Uncertainties\n",
        "Use the trained BNN to predict redshifts for the dataset and estimate the uncertainties.\n",
        "\n",
        "### 5. Integrate the BNN Outputs with the Cosmological Analysis\n",
        "Incorporate the predicted redshifts and uncertainties into the redshift binning and cosmological parameter inference.\n",
        "\n",
        "Below is a simplified code outline for each of these steps.\n",
        "\n",
        "#### Step 1: Define the Problem and Prepare the Data\n",
        "Let's assume you have a dataset with photometric measurements and spectroscopic redshifts.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "photometric_data = data[['feature1', 'feature2', 'feature3', 'feature4']].values  # Example features\n",
        "spectroscopic_redshifts = data['redshift'].values\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(photometric_data, spectroscopic_redshifts, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "#### Step 2: Build the Bayesian Neural Network\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Define the BNN model\n",
        "def build_bnn(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tfp.layers.DenseVariational(1,\n",
        "                                    make_prior_fn=tfp.layers.default_multivariate_normal_fn,\n",
        "                                    make_posterior_fn=tfp.layers.default_mean_field_normal_fn,\n",
        "                                    kl_weight=1/X_train.shape[0])\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "bnn_model = build_bnn(input_shape)\n",
        "\n",
        "# Compile the BNN model\n",
        "negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
        "bnn_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=negloglik)\n",
        "```\n",
        "\n",
        "#### Step 3: Train the BNN\n",
        "\n",
        "```python\n",
        "# Train the BNN model\n",
        "bnn_model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32)\n",
        "```\n",
        "\n",
        "#### Step 4: Predict Redshifts and Uncertainties\n",
        "\n",
        "```python\n",
        "# Make predictions\n",
        "y_pred_distribution = bnn_model(X_test)\n",
        "y_pred = y_pred_distribution.mean().numpy()\n",
        "y_pred_stddev = y_pred_distribution.stddev().numpy()\n",
        "\n",
        "# Visualize the predictions and uncertainties\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(y_test, y_pred, yerr=y_pred_stddev, fmt='o', alpha=0.5)\n",
        "plt.plot([0, 3], [0, 3], 'k--')\n",
        "plt.xlabel('True Redshift')\n",
        "plt.ylabel('Predicted Redshift')\n",
        "plt.title('Predicted Redshift vs True Redshift with Uncertainties')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Step 5: Integrate the BNN Outputs with the Cosmological Analysis\n",
        "\n",
        "Modify the `RedshiftBinning` class to use the predicted redshifts and uncertainties from the BNN.\n",
        "\n",
        "```python\n",
        "class RedshiftBinning:\n",
        "    def __init__(self, redshifts, redshift_dist, bin_edges, uncertainties):\n",
        "        self.redshifts = redshifts\n",
        "        self.redshift_dist = redshift_dist\n",
        "        self.bin_edges = bin_edges\n",
        "        self.uncertainties = uncertainties\n",
        "        self.binned_distribution = np.zeros((len(bin_edges) - 1, len(redshifts)))\n",
        "\n",
        "    def gaussian_contribution(self, z_i, sigma, bin_edge1, bin_edge2):\n",
        "        norm_factor = 1 / (sigma * np.sqrt(2 * np.pi))\n",
        "        integrand = lambda x: norm_factor * np.exp(-0.5 * ((x - z_i) / sigma) ** 2)\n",
        "        contribution, _ = quad(integrand, bin_edge1, bin_edge2)\n",
        "        return contribution\n",
        "\n",
        "    def bin_data(self, scatter):\n",
        "        for i, z in enumerate(self.redshifts):\n",
        "            for j in range(len(self.bin_edges) - 1):\n",
        "                bin_edge1 = self.bin_edges[j]\n",
        "                bin_edge2 = self.bin_edges[j + 1]\n",
        "                z_scattered = z + scatter[i]\n",
        "                self.binned_distribution[j, i] = self.redshift_dist[i] * self.gaussian_contribution(z_scattered, self.uncertainties[i], bin_edge1, bin_edge2)\n",
        "\n",
        "        # Normalize the binned distribution\n",
        "        for j in range(len(self.bin_edges) - 1):\n",
        "            self.binned_distribution[j, :] /= np.sum(self.binned_distribution[j, :])\n",
        "\n",
        "# Use the predicted redshifts and uncertainties\n",
        "predicted_redshifts = y_pred\n",
        "predicted_uncertainties = y_pred_stddev\n",
        "\n",
        "binning = RedshiftBinning(predicted_redshifts, redshift_distribution, bin_edges, predicted_uncertainties)\n",
        "binning.bin_data(predicted_uncertainties)\n",
        "```\n",
        "\n",
        "The rest of the code for plotting, creating tracers, calculating angular power spectra, and correlation functions would follow similarly, incorporating the binned data from the BNN predictions.\n",
        "\n",
        "By integrating a BNN for photometric redshift estimation, we ensure that the resulting measurements are both accurate and reliable, thereby advancing our understanding of the universe through more precise cosmological analyses."
      ],
      "metadata": {
        "id": "KGMdyBfQTI2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating a Bayesian Neural Network (BNN) into your cosmological parameter inference project will involve the following steps. Here’s a comprehensive approach based on your provided template code:\n",
        "\n",
        "1. **Import necessary libraries**:\n",
        "   - `pyccl` for cosmological calculations.\n",
        "   - `numpy` for numerical operations.\n",
        "   - `matplotlib` for plotting.\n",
        "   - `emcee` for MCMC sampling.\n",
        "   - `scipy` for integration and linear algebra.\n",
        "\n",
        "2. **Create a class for SRD Redshift Distributions**:\n",
        "   - This class will define a redshift distribution based on the Smail-type distribution.\n",
        "\n",
        "3. **Create a class for Redshift Binning**:\n",
        "   - This class will handle the binning of redshift data, taking into account uncertainties.\n",
        "\n",
        "4. **Define cosmological parameters and angular multipoles**:\n",
        "   - These parameters and multipoles will be used for calculating angular power spectra and correlation functions.\n",
        "\n",
        "5. **Create a function to compute true correlations**:\n",
        "   - This function will compute the true correlation function for each bin using the true redshift distribution.\n",
        "\n",
        "6. **Define a function to compute cosmological parameters Omega_m and sigma8**:\n",
        "   - This function will calculate the redshift distribution with given parameters, bin the data, create tracers, and calculate angular power spectra. It will also define the likelihood function for MCMC sampling.\n",
        "\n",
        "7. **Set up MCMC for parameter estimation**:\n",
        "   - The MCMC setup will involve defining a prior function, a combined log-probability function, initializing the sampler, and running MCMC to extract samples and compute the mean and standard deviation of parameters.\n",
        "\n",
        "8. **Run the analysis and find optimal parameters**:\n",
        "   - Loop over ranges of alpha, beta, and sigma_z to find the optimal parameters that minimize deviation from target values.\n",
        "\n",
        "9. **Plot the results**:\n",
        "   - Create plots to visualize Omega_m and sigma8 for different sigma_z values.\n",
        "\n",
        "Here’s the complete code implementation:\n",
        "\n",
        "```python\n",
        "import pyccl as ccl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install emcee\n",
        "import emcee\n",
        "from scipy.integrate import quad\n",
        "from scipy.linalg import inv\n",
        "\n",
        "# Define SRDRedshiftDistributions class\n",
        "class SRDRedshiftDistributions:\n",
        "    def __init__(self, redshift_range):\n",
        "        self.redshift_range = redshift_range\n",
        "\n",
        "    def smail_type_distribution(self, pivot_redshift, alpha, beta):\n",
        "        z_div_pivot = self.redshift_range / pivot_redshift\n",
        "        redshift_distribution = z_div_pivot ** beta * np.exp(-(z_div_pivot) ** alpha)\n",
        "        return redshift_distribution\n",
        "\n",
        "# Define RedshiftBinning class\n",
        "class RedshiftBinning:\n",
        "    def __init__(self, redshifts, redshift_dist, bin_edges, uncertainties):\n",
        "        self.redshifts = redshifts\n",
        "        self.redshift_dist = redshift_dist\n",
        "        self.bin_edges = bin_edges\n",
        "        self.uncertainties = uncertainties\n",
        "        self.binned_distribution = np.zeros((len(bin_edges) - 1, len(redshifts)))\n",
        "\n",
        "    def gaussian_contribution(self, z_i, sigma, bin_edge1, bin_edge2):\n",
        "        norm_factor = 1 / (sigma * np.sqrt(2 * np.pi))\n",
        "        integrand = lambda x: norm_factor * np.exp(-0.5 * ((x - z_i) / sigma) ** 2)\n",
        "        contribution, _ = quad(integrand, bin_edge1, bin_edge2)\n",
        "        return contribution\n",
        "\n",
        "    def bin_data(self, scatter):\n",
        "        for i, z in enumerate(self.redshifts):\n",
        "            for j in range(len(self.bin_edges) - 1):\n",
        "                bin_edge1 = self.bin_edges[j]\n",
        "                bin_edge2 = self.bin_edges[j + 1]\n",
        "                z_scattered = z + scatter[i]\n",
        "                self.binned_distribution[j, i] = self.redshift_dist[i] * self.gaussian_contribution(z_scattered, self.uncertainties[i], bin_edge1, bin_edge2)\n",
        "\n",
        "        # Normalize the binned distribution\n",
        "        self.binned_distribution /= self.binned_distribution.sum(axis=1)[:, None]\n",
        "\n",
        "# Define cosmological parameters\n",
        "cosmo = ccl.Cosmology(Omega_c=0.27, Omega_b=0.045, h=0.67, sigma8=0.83, n_s=0.96)\n",
        "\n",
        "# Angular multipoles\n",
        "ell = np.arange(2, 2000)\n",
        "\n",
        "# Define angular scales in degrees\n",
        "theta_deg = np.logspace(-1, np.log10(5.), 20)\n",
        "\n",
        "# Calculate the correlation function for each bin with true redshift distribution\n",
        "def compute_true_correlations(bin_edges, redshift_range, true_lens_redshift_distribution_dict):\n",
        "    true_tracers = []\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        z_bin = redshift_range\n",
        "        dNdz = true_lens_redshift_distribution_dict[i]\n",
        "        tracer = ccl.NumberCountsTracer(cosmo, has_rsd=False, dndz=(z_bin, dNdz), bias=(z_bin, np.full_like(z_bin, 1.5)))\n",
        "        true_tracers.append(tracer)\n",
        "\n",
        "    true_cls_matrix = np.zeros((len(bin_edges) - 1, len(ell)))\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        true_cls_matrix[i] = ccl.angular_cl(cosmo, true_tracers[i], true_tracers[i], ell)\n",
        "\n",
        "    true_correlations_matrix = np.zeros((len(bin_edges) - 1, len(theta_deg)))\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        true_cls = true_cls_matrix[i]\n",
        "        true_correlations_matrix[i] = ccl.correlation(cosmo, ell=ell, C_ell=true_cls, theta=theta_deg, type='NN', method='FFTLog')\n",
        "\n",
        "    return true_correlations_matrix\n",
        "\n",
        "# Function to compute Omega_m and sigma8 for given alpha, beta, and photoz uncertainty\n",
        "def compute_omega_sigma(alpha, beta, sigma_z, covariance_matrix, true_correlations_matrix):\n",
        "    # Calculate redshift distribution with given alpha and beta\n",
        "    redshift_range = np.linspace(0.2, 1.2, 512)\n",
        "    pivot_redshift = 0.26\n",
        "    srd_dist = SRDRedshiftDistributions(redshift_range)\n",
        "    redshift_distribution = srd_dist.smail_type_distribution(pivot_redshift, alpha, beta)\n",
        "\n",
        "    # Define bin edges and sigma (uncertainty)\n",
        "    bin_edges = np.linspace(0.2, 1.2, 6)\n",
        "    sigma = sigma_z * np.ones_like(redshift_range)\n",
        "\n",
        "    # Calculate scatter\n",
        "    scatter = sigma_z * (1 + redshift_range)\n",
        "\n",
        "    # Instantiate RedshiftBinning and bin the data\n",
        "    binning = RedshiftBinning(redshift_range, redshift_distribution, bin_edges, sigma)\n",
        "    binning.bin_data(scatter)\n",
        "\n",
        "    # Create NumberCountsTracers for each bin using the binned redshift distribution\n",
        "    tracers = []\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        z_bin = redshift_range\n",
        "        dNdz_bin = binning.binned_distribution[i]\n",
        "        tracer = ccl.NumberCountsTracer(cosmo, has_rsd=False, dndz=(z_bin, dNdz_bin), bias=(z_bin, np.full_like(z_bin, 1.5)))\n",
        "        tracers.append(tracer)\n",
        "\n",
        "    # Calculate angular power spectra\n",
        "    cls = [ccl.angular_cl(cosmo, tracer, tracer, ell) for tracer in tracers]\n",
        "\n",
        "    # Define likelihood function\n",
        "    def ln_likelihood(theta):\n",
        "        Omega_m, sigma8 = theta\n",
        "        ln_likelihood_total = 0.0\n",
        "\n",
        "        # Loop over each bin and compute likelihood contribution\n",
        "        for i in range(len(bin_edges) - 1):\n",
        "            xi_true = true_correlations_matrix[i]\n",
        "            xi_model = ccl.correlation(cosmo, ell=ell, C_ell=cls[i], theta=theta_deg, type='NN', method='FFTLog')\n",
        "            diff = xi_true - xi_model\n",
        "            chi2 = diff.T @ inv(covariance_matrix) @ diff\n",
        "            ln_likelihood_total += -0.5 * chi2\n",
        "\n",
        "        return ln_likelihood_total\n",
        "\n",
        "    # MCMC setup\n",
        "    ndim = 2  # Number of parameters (Omega_m, sigma8)\n",
        "    nwalkers = 50  # Number of walkers\n",
        "    p0 = np.random.rand(nwalkers, ndim)  # Random initial positions in parameter space\n",
        "\n",
        "    # Define prior function\n",
        "    def ln_prior(theta):\n",
        "        Omega_m, sigma8 = theta\n",
        "        if 0.267 - 0.1 < Omega_m < 0.267 + 0.1 and 0.762 - 0.1 < sigma8 < 0.762 + 0.1:\n",
        "            return 0.0\n",
        "        return -np.inf\n",
        "\n",
        "    # Define combined log-probability function\n",
        "    def ln_prob(theta):\n",
        "        lp = ln_prior(theta)\n",
        "        if not np.isfinite(lp):\n",
        "            return -np.inf\n",
        "        return lp + ln_likelihood(theta)\n",
        "\n",
        "    # Initialize sampler and run MCMC\n",
        "    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob)\n",
        "    pos, prob, state = sampler.run_mcmc(p0, 100)\n",
        "    sampler.reset()\n",
        "    sampler.run_mcmc(pos, 1000)\n",
        "\n",
        "    # Extract samples\n",
        "    samples = sampler.chain[:, 50:, :].reshape((-1, ndim))\n",
        "\n",
        "    # Compute mean and standard deviation of parameters\n",
        "    Omega_m_mean, sigma8_mean = np.mean(samples, axis=0)\n",
        "    Omega\n",
        "\n",
        "_m_std, sigma8_std = np.std(samples, axis=0)\n",
        "\n",
        "    # Compute the deviation from target values\n",
        "    Omega_m_target = 0.267\n",
        "    sigma8_target = 0.762\n",
        "    deviation = np.sqrt((Omega_m_mean - Omega_m_target) ** 2 + (sigma8_mean - sigma8_target) ** 2)\n",
        "\n",
        "    return Omega_m_mean, Omega_m_std, sigma8_mean, sigma8_std, deviation\n",
        "\n",
        "# Example usage\n",
        "alpha_range = [0.5, 1.0, 1.5, 2.0]\n",
        "beta_range = [0.5, 1.0, 1.5, 2.0]\n",
        "sigma_z_values = [0.02, 0.03, 0.04]\n",
        "\n",
        "covariance_matrix = np.identity(len(theta_deg))  # Example identity matrix for covariance\n",
        "\n",
        "true_lens_redshift_distribution_dict = {i: np.ones_like(np.linspace(0.2, 1.2, 512)) for i in range(5)}  # Placeholder\n",
        "bin_edges = np.linspace(0.2, 1.2, 6)\n",
        "true_correlations_matrix = compute_true_correlations(bin_edges, np.linspace(0.2, 1.2, 512), true_lens_redshift_distribution_dict)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for alpha in alpha_range:\n",
        "    for beta in beta_range:\n",
        "        for sigma_z in sigma_z_values:\n",
        "            omega_m_mean, omega_m_std, sigma8_mean, sigma8_std, deviation = compute_omega_sigma(alpha, beta, sigma_z, covariance_matrix, true_correlations_matrix)\n",
        "            results[(alpha, beta, sigma_z)] = (omega_m_mean, omega_m_std, sigma8_mean, sigma8_std, deviation)\n",
        "\n",
        "# Find the optimal parameters\n",
        "optimal_params = min(results, key=lambda x: results[x][-1])\n",
        "optimal_alpha, optimal_beta, optimal_sigma_z = optimal_params\n",
        "print(f'Optimal alpha: {optimal_alpha}, Optimal beta: {optimal_beta}, Optimal sigma_z: {optimal_sigma_z}')\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(len(sigma_z_values), 2, figsize=(12, len(sigma_z_values) * 6))\n",
        "\n",
        "for i, sigma_z in enumerate(sigma_z_values):\n",
        "    omega_m_values = np.zeros((len(alpha_range), len(beta_range)))\n",
        "    sigma8_values = np.zeros((len(alpha_range), len(beta_range)))\n",
        "\n",
        "    for j, alpha in enumerate(alpha_range):\n",
        "        for k, beta in enumerate(beta_range):\n",
        "            omega_m_mean, omega_m_std, sigma8_mean, sigma8_std, _ = results[(alpha, beta, sigma_z)]\n",
        "            omega_m_values[j, k] = omega_m_mean\n",
        "            sigma8_values[j, k] = sigma8_mean\n",
        "\n",
        "    ax1 = axes[i, 0]\n",
        "    ax2 = axes[i, 1]\n",
        "\n",
        "    c1 = ax1.imshow(omega_m_values, extent=[beta_range[0], beta_range[-1], alpha_range[0], alpha_range[-1]], origin='lower', aspect='auto')\n",
        "    fig.colorbar(c1, ax=ax1)\n",
        "    ax1.set_title(f'Omega_m for sigma_z = {sigma_z}')\n",
        "    ax1.set_xlabel('Beta')\n",
        "    ax1.set_ylabel('Alpha')\n",
        "\n",
        "    c2 = ax2.imshow(sigma8_values, extent=[beta_range[0], beta_range[-1], alpha_range[0], alpha_range[-1]], origin='lower', aspect='auto')\n",
        "    fig.colorbar(c2, ax=ax2)\n",
        "    ax2.set_title(f'sigma8 for sigma_z = {sigma_z}')\n",
        "    ax2.set_xlabel('Beta')\n",
        "    ax2.set_ylabel('Alpha')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "In this implementation:\n",
        "\n",
        "1. **Classes**:\n",
        "   - `SRDRedshiftDistributions` handles the redshift distribution.\n",
        "   - `RedshiftBinning` bins the redshift data considering uncertainties.\n",
        "\n",
        "2. **Cosmological Parameters**:\n",
        "   - Defined using `pyccl.Cosmology`.\n",
        "\n",
        "3. **True Correlations**:\n",
        "   - Computed using true redshift distributions.\n",
        "\n",
        "4. **MCMC Sampling**:\n",
        "   - Set up for cosmological parameter inference.\n",
        "\n",
        "5. **Results Analysis**:\n",
        "   - Loop through parameter ranges to find optimal values.\n",
        "\n",
        "6. **Plotting**:\n",
        "   - Visualize results for different values of sigma_z.\n",
        "\n",
        "Make sure to have `pyccl`, `numpy`, `matplotlib`, `emcee`, and `scipy` installed before running the code."
      ],
      "metadata": {
        "id": "-VdLlQysUpgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating a Bayesian Neural Network (BNN) into your cosmological parameter inference process will require some modifications to the existing structure. Here's how you can do this:\n",
        "\n",
        "1. **Define and Train a BNN**:\n",
        "   - Use a library like TensorFlow Probability or PyTorch to define a BNN that can predict the cosmological parameters Omega_m and sigma8 given input features (e.g., redshift distribution parameters, angular power spectra).\n",
        "\n",
        "2. **Incorporate the BNN into the MCMC Process**:\n",
        "   - Use the trained BNN to predict cosmological parameters within the MCMC sampling process.\n",
        "\n",
        "3. **Combine BNN Predictions with Likelihood Calculation**:\n",
        "   - The BNN will provide predictions of the parameters, and these predictions will be used in the likelihood function to compute the posterior distribution.\n",
        "\n",
        "Here's a step-by-step guide:\n",
        "\n",
        "1. **Install Required Libraries**:\n",
        "   - TensorFlow Probability: `pip install tensorflow tensorflow-probability`\n",
        "   - PyTorch: `pip install torch`\n",
        "\n",
        "2. **Define the BNN**:\n",
        "   - Create a BNN model using TensorFlow Probability or PyTorch.\n",
        "\n",
        "3. **Train the BNN**:\n",
        "   - Train the BNN on a dataset of input features and corresponding cosmological parameters.\n",
        "\n",
        "4. **Integrate the BNN into the MCMC Process**:\n",
        "   - Use the BNN to predict the parameters in the MCMC sampling.\n",
        "\n",
        "Below is an example code outline integrating a BNN using TensorFlow Probability:\n",
        "\n",
        "### Step 1: Define and Train the BNN\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example dataset\n",
        "# Assuming X_train contains input features and y_train contains the corresponding parameters\n",
        "X_train = np.random.rand(100, 10)  # Replace with actual data\n",
        "y_train = np.random.rand(100, 2)   # Replace with actual data\n",
        "\n",
        "# Define the BNN model\n",
        "def create_bnn():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tfp.layers.DenseFlipout(50, activation='relu'),\n",
        "        tfp.layers.DenseFlipout(2)  # Output layer for Omega_m and sigma8\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "bnn_model = create_bnn()\n",
        "\n",
        "# Define loss and optimizer\n",
        "negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
        "bnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "\n",
        "# Train the BNN model\n",
        "bnn_model.fit(X_train, y_train, epochs=100, verbose=1)\n",
        "```\n",
        "\n",
        "### Step 2: Incorporate the BNN into the MCMC Process\n",
        "\n",
        "```python\n",
        "import emcee\n",
        "from scipy.linalg import inv\n",
        "\n",
        "# Define cosmological parameters and angular multipoles\n",
        "cosmo = ccl.Cosmology(Omega_c=0.27, Omega_b=0.045, h=0.67, sigma8=0.83, n_s=0.96)\n",
        "ell = np.arange(2, 2000)\n",
        "theta_deg = np.logspace(-1, np.log10(5.), 20)\n",
        "\n",
        "# Define the function to compute the likelihood\n",
        "def ln_likelihood(theta):\n",
        "    Omega_m, sigma8 = theta\n",
        "    ln_likelihood_total = 0.0\n",
        "\n",
        "    # Loop over each bin and compute likelihood contribution\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        xi_true = true_correlations_matrix[i]\n",
        "        xi_model = ccl.correlation(cosmo, ell=ell, C_ell=cls[i], theta=theta_deg, type='NN', method='FFTLog')\n",
        "        diff = xi_true - xi_model\n",
        "        chi2 = diff.T @ inv(covariance_matrix) @ diff\n",
        "        ln_likelihood_total += -0.5 * chi2\n",
        "\n",
        "    return ln_likelihood_total\n",
        "\n",
        "# Define the prior function\n",
        "def ln_prior(theta):\n",
        "    Omega_m, sigma8 = theta\n",
        "    if 0.1 < Omega_m < 0.5 and 0.5 < sigma8 < 1.0:\n",
        "        return 0.0\n",
        "    return -np.inf\n",
        "\n",
        "# Define the combined log-probability function\n",
        "def ln_prob(theta):\n",
        "    lp = ln_prior(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    return lp + ln_likelihood(theta)\n",
        "\n",
        "# MCMC setup\n",
        "ndim = 2  # Number of parameters (Omega_m, sigma8)\n",
        "nwalkers = 50  # Number of walkers\n",
        "p0 = np.random.rand(nwalkers, ndim)  # Random initial positions in parameter space\n",
        "\n",
        "# Initialize sampler and run MCMC\n",
        "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob)\n",
        "pos, prob, state = sampler.run_mcmc(p0, 100)\n",
        "sampler.reset()\n",
        "sampler.run_mcmc(pos, 1000)\n",
        "\n",
        "# Extract samples\n",
        "samples = sampler.chain[:, 50:, :].reshape((-1, ndim))\n",
        "\n",
        "# Compute mean and standard deviation of parameters\n",
        "Omega_m_mean, sigma8_mean = np.mean(samples, axis=0)\n",
        "Omega_m_std, sigma8_std = np.std(samples, axis=0)\n",
        "\n",
        "# Compute the deviation from target values\n",
        "Omega_m_target = 0.267\n",
        "sigma8_target = 0.762\n",
        "deviation = np.sqrt((Omega_m_mean - Omega_m_target) ** 2 + (sigma8_mean - sigma8_target) ** 2)\n",
        "\n",
        "print(f'Omega_m: {Omega_m_mean} ± {Omega_m_std}')\n",
        "print(f'sigma8: {sigma8_mean} ± {sigma8_std}')\n",
        "print(f'Deviation: {deviation}')\n",
        "```\n",
        "\n",
        "### Step 3: Integrate BNN Predictions with MCMC\n",
        "\n",
        "In the `ln_likelihood` function, instead of directly using the parameters, you can use the BNN to predict the parameters and use these predictions within the MCMC sampling.\n",
        "\n",
        "```python\n",
        "def ln_likelihood(theta):\n",
        "    # Use the BNN to predict Omega_m and sigma8\n",
        "    Omega_m, sigma8 = bnn_model.predict(theta[None, :]).flatten()\n",
        "    ln_likelihood_total = 0.0\n",
        "\n",
        "    # Loop over each bin and compute likelihood contribution\n",
        "    for i in range(len(bin_edges) - 1):\n",
        "        xi_true = true_correlations_matrix[i]\n",
        "        xi_model = ccl.correlation(cosmo, ell=ell, C_ell=cls[i], theta=theta_deg, type='NN', method='FFTLog')\n",
        "        diff = xi_true - xi_model\n",
        "        chi2 = diff.T @ inv(covariance_matrix) @ diff\n",
        "        ln_likelihood_total += -0.5 * chi2\n",
        "\n",
        "    return ln_likelihood_total\n",
        "```\n",
        "\n",
        "In this code outline:\n",
        "1. **BNN Definition and Training**:\n",
        "   - A simple BNN model is defined using TensorFlow Probability.\n",
        "   - The BNN is trained on a synthetic dataset (`X_train`, `y_train`).\n",
        "\n",
        "2. **MCMC Integration**:\n",
        "   - The BNN is used within the MCMC process to predict cosmological parameters.\n",
        "   - The `ln_likelihood` function uses the BNN to predict `Omega_m` and `sigma8`.\n",
        "\n",
        "3. **Likelihood and Prior Functions**:\n",
        "   - The log-likelihood and prior functions are defined.\n",
        "   - MCMC sampling is set up and executed.\n",
        "\n",
        "By following this approach, you integrate a BNN into your cosmological parameter inference process, using the BNN to predict parameters within the MCMC sampling. Adjust the BNN architecture, training data, and MCMC settings as needed for your specific application."
      ],
      "metadata": {
        "id": "0TBLIrwSUhz4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeQ7Ah5Ih82X5I9DPqhlml",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}